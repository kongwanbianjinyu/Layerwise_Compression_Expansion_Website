<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>From Compression to Expansion: A Layerwise Analysis of In-Context Learning</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <style>
    .equation-box {
      background-color: #f5f5f5;
      border-radius: 6px;
      padding: 20px;
      margin: 20px 0;
      font-size: 1.25em;
      overflow-x: auto;
      text-align: center;
    }
    .phenomenon-box {
      background-color: #f8f9fa;
      border: 2px solid #007bff;
      border-radius: 10px;
      padding: 20px;
      margin: 20px 0 10px 0;
      box-shadow: 0 4px 6px rgba(0, 123, 255, 0.1);
    }
    .phenomenon-title {
      font-size: 1.3em;
      font-weight: bold;
      color: #007bff;
      margin-bottom: 12px;
      text-align: center;
    }
    .phenomenon-text {
      line-height: 1.5;
      color: #333;
      text-align: justify;
      font-weight: bold;
    }
    .author-block a {
      color: #007bff !important;
      text-decoration: none;
    }
    .author-block a:hover {
      color: #0056b3 !important;
      text-decoration: underline;
    }
  </style>

  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body" style="padding-bottom: 1rem;">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">From Compression to Expansion:<br>
              A Layerwise Analysis of In-Context Learning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://kongwanbianjinyu.github.io/" target="_blank">Jiachen Jiang</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=yFJv-2kAAAAJ&hl=en" target="_blank">Yuxin Dong</a>,</span>
                  <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=XR5CQJcAAAAJ&hl=en" target="_blank">Jinxin Zhou</a>,</span>

                  <span class="author-block">
                  <a href="https://zhihuizhu.github.io/" target="_blank">Zhihui Zhu</a><sup>â€ </sup>,</span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">The Ohio State University<br></span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates First Author, <sup>â€ </sup>Indicates Corresponding Author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://www.arxiv.org/pdf/2505.17322" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://www.arxiv.org/abs/2505.17322" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                  </a>
                </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/kongwanbianjinyu/Compression_to_Expansion" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Demo link -->
                <!-- <span class="link-block">
                  <a href="https://huggingface.co/spaces/jiachenjiang/Cat-AIR" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-play"></i>
                  </span>
                  <span>Demo ðŸ¤—</span>
                  </a>
                </span> -->

                <!-- Models link -->
                <!-- <span class="link-block">
                  <a href="https://huggingface.co/collections/jiachenjiang/cat-air-6703f63c41373954c1f36aa0" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-cube"></i>
                  </span>
                  <span>Models ðŸ¤—</span>
                  </a>
                </span> -->

                
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Phenomenon Highlight Box -->
<section class="hero is-small">
  <div class="hero-body" style="padding: 0.5rem 1.5rem;">
    <div class="container is-fullhd">
      <div class="columns is-centered">
        <div class="column is-10">
          <div class="phenomenon-box">
            <div class="phenomenon-title">Layerwise Compression-Expansion Phenomenon</div>
            <div class="phenomenon-text">
              LLMs exhibiting ICL capabilities organize their layers into two parts with distinct behaviors: a compression part and an expansion part. The early layers, comprising the compression part, progressively produce compact and discriminative representations that capture task information from the input demonstrations. The later layers, forming the expansion part, apply these compact representations to the query to generate the output.
            </div>
            <div class="image-container" style="text-align: center; margin-top: 20px;">
              <img src="static/images/layerwise_accuracy_cdnv.png" alt="Overview Image" style="max-width: 90%; height: auto; border-radius: 8px;">
            </div>
            <p class="has-text-centered" style="font-size: 0.85em; margin-top: 12px; color: #666; font-style: normal;">
              Layer-wise compression to expansion in ICL representations. TDNV first decreases then increases from shallow to deep layers, splitting the model into compression and expansion stages. During the compression stage, task vector accuracy increases as task information is progressively extracted from demonstration pairs. During the expansion stage, early-exit accuracy increases as output information is progressively decoded based on the input query.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- TL;DR Section -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <div class="content has-text-centered">
            <h2 class="title">TL;DR</h2>
            <p>
              We uncover a universal <strong>Compression-to-Expansion</strong> ðŸš€ pattern in ICL representations, revealing how LLMs extract and utilize task information across layers.
            </p>
            <p>
              ðŸ”¥ðŸ”¥ More content comming soon: codes, demos.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Intro Video -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="publication-video">
        <video controls width="100%" muted>
          <source src="static/videos/0619.mov" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
    </div>
  </div>
</section>










<!-- Paper abstract -->
<!-- <section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In-context learning (ICL) enables large language models (LLMs) to adapt to new
tasks without weight updates by learning from demonstration sequences. While
ICL shows strong empirical performance, its internal representational mechanisms
are not yet well understood. In this work, we conduct a statistical geometric analysis of ICL representations to investigate how task-specific information is captured
across layers. Our analysis reveals an intriguing phenomenon, which we term Layerwise Compression-Expansion: early layers progressively produce compact and
discriminative representations that encode task information from the input demonstrations, while later layers expand these representations to incorporate the query
and generate the prediction. This phenomenon is observed consistently across diverse tasks and a range of contemporary LLM architectures. We demonstrate that
it has important implications for ICL performanceâ€”improving with model size
and the number of demonstrationsâ€”and for robustness in the presence of noisy
examples. To further understand the effect of the compact task representation, we
propose a bias-variance decomposition and provide a theoretical analysis showing
how attention mechanisms contribute to reducing both variance and bias, thereby
enhancing performance as the number of demonstrations increases. Our findings
reveal an intriguing layerwise dynamic in ICL, highlight how structured representations emerge within LLMs, and showcase that analyzing internal representations
can facilitate a deeper understanding of model behavior.
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End paper abstract -->

<!-- Tasks  -->
<section class="section hero is-light">
  <div class="hero-body">
    <div class="columns is-centered has-text-centered">
      <div class="column is-half">
        <h2 class="title is-3">In-Context Learning Tasks</h2>
        <div class="content">
          <figure class="image">
            <img src="static/images/tasks.png" alt="Tasks Visualization">

          </figure>
        </div>
      </div>
    </div>
  </div>
</section>






<!-- Method Section -->
<section class="section hero is-light" >
  <div class="hero-body">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">TDNV: Metric for Representation Compression</h2>
        <div class="content has-text-justified">
          <p>
            We analyze model representations for in-context learning (ICL). For each task, we use the hidden representation $h_{i,t}^{(l)}$ from each sample $i$ of task $t$ in layer $l$.
          </p>
          <p>
            To measure representation quality, we propose the <b>Task-Distance Normalized Variance (TDNV)</b>. It is the ratio of within-task variance to between-task distance. Lower TDNV means more compressed representations.
          </p>
          <div class="equation-box">
$$\text{TDNV}^{(l)} := \sum_{t=1}^{T} \sum_{\substack{t'=1 \\ t' \neq t}}^{T} \frac{\text{var}_t^{(l)} + \text{var}_{t'}^{(l)}}{2\|\bar{h}_t^{(l)} - \bar{h}_{t'}^{(l)}\|_2^2}$$
          </div>
          <p>TDNV has two main components:</p>
          <ul>
            <li style="text-align: left; margin-bottom: 1em;">
              <b>Within-task variance($\text{var}_t^{(l)}$).</b> It measures how tightly examples from the same task cluster. A smaller variance means better compression.
              <div class="equation-box">
$$\text{var}_t^{(l)} = \frac{1}{N} \sum_{i=1}^{N} \|h_{i,t}^{(l)} - \bar{h}_t^{(l)}\|_2^2, \quad \text{where} \quad \bar{h}_t^{(l)} = \frac{1}{N} \sum_{i=1}^{N} h_{i,t}^{(l)}.$$
              </div>
            </li>
            <li style="text-align: left;">
              <b>Between-task distance($\|\bar{h}_t^{(l)} - \bar{h}_{t'}^{(l)}\|_2$).</b> It measures the separation between different tasks. A larger distance means better separation.
            </li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Experimental Results Section -->
<section class="section hero is-light">
  <div class="hero-body">
    <div class="container is-fullhd">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <h2 class="title is-2">Experimental Results</h2>
          
          <!-- Prevalence of Phenomenon Subsection -->
          <div style="margin-top: 2rem;">
            <h3 class="title is-4" style="color: #007bff; margin-bottom: 1.5rem;">Prevalence of Phenomenon</h3>
            
            <p style="font-size: 1.1em; color: #555; text-align: center; margin-bottom: 1.5rem;">
              Take Away: The compression-expansion phenomenon is universal across model architectures and emerges naturally during training.
            </p>
            
            <div class="columns is-centered">
              <div class="column is-5">
                <div class="content">
                  <figure class="image">
                    <img src="static/images/cdnv_models.png" alt="Results Image" style="width: 90%; margin: 0 auto; display: block;">
                    <figcaption style="margin-top: 10px; font-size: 0.9em; color: #666;">
                      Layerwise TDNV of different model architectures, including transformer and state space model.
                    </figcaption>
                  </figure>
                </div>
              </div>
              <div class="column is-5">
                <div class="content">
                  <figure class="image">
                    <img src="static/images/amber_checkpoints_tdnv_comparison.png" alt="Additional Results" style="width: 90%; margin: 0 auto; display: block;">
                    <figcaption style="margin-top: 10px; font-size: 0.9em; color: #666;">
                       Layerwise TDNV during training process. The phenomenon emerges and intensifies with training progress.
                    </figcaption>
                  </figure>
                </div>
              </div>
            </div>
          </div>
          
          <!-- Scaling Up Model Size Subsection -->
          <div style="margin-top: 3rem;">
            <h3 class="title is-4" style="color: #007bff; margin-bottom: 1.5rem;">Scaling Up Model Size Leads to More Compression</h3>
            
            <div class="columns is-centered">
              <div class="column is-fullwidth">
                <div class="content has-text-centered">
                  <p style="margin-bottom: 1.5rem; font-size: 1.1em; color: #555;">
                    Take Away: As model size increases, the phenomenon becomes more pronounced, with larger models achieving better task representation compression.
                  </p>
                  
                  <div class="columns is-centered">
                    <div class="column is-5">
                      <figure class="image">
                        <img src="static/images/cdnv_pythia_small.png" alt="Model Scaling Results 1" style="width: 90%; border-radius: 8px; margin: 0 auto; display: block;">
                        <figcaption style="margin-top: 10px; font-size: 0.9em; color: #666;">
                          Layerwise TDNV of varying model size.
                        </figcaption>
                      </figure>
                    </div>
                    <div class="column is-5">
                      <figure class="image">
                        <img src="static/images/pythia_mean_accuracy_cdnv.png" alt="Model Scaling Results 2" style="width: 90%; border-radius: 8px; margin: 0 auto; display: block;">
                        <figcaption style="margin-top: 10px; font-size: 0.9em; color: #666;">
                          ICL Performance v.s. minimum TDNV of varying model size.
                        </figcaption>
                      </figure>
                    </div>
                  </div>
                  
                </div>
              </div>
            </div>
          </div>
          
          <!-- Compression-to-Expansion under Noisy Demonstrations Subsection -->
          <div style="margin-top: 3rem;">
            <h3 class="title is-4" style="color: #007bff; margin-bottom: 1.5rem;">Compression-to-Expansion under Noisy Demonstrations</h3>
            
            <p style="font-size: 1.1em; color: #555; text-align: center; margin-bottom: 1.5rem;">
              Take Away: As the noise ratio increases, TDNV rises, and once the within-task variance exceeds the between-task distance ($\text{TDNV} > 1$), ICL performance drops sharply.
            </p>
            
            <div class="columns is-centered">
              <div class="column is-fullwidth">
                <div class="content has-text-centered">
                  <div class="columns is-centered">
                    <div class="column is-5">
                      <figure class="image">
                        <img src="static/images/acc_noise.png" alt="Noisy Demonstrations Results 1" style="width: 90%; border-radius: 8px; margin: 0 auto; display: block;">
                        <figcaption style="margin-top: 10px; font-size: 0.9em; color: #666;">
                          ICL Performance under different noise ratios.
                        </figcaption>
                      </figure>
                    </div>
                    <div class="column is-5">
                      <figure class="image">
                        <img src="static/images/cdnv_perturb.png" alt="Noisy Demonstrations Results 2" style="width: 90%; border-radius: 8px; margin: 0 auto; display: block;">
                        <figcaption style="margin-top: 10px; font-size: 0.9em; color: #666;">
                          Layerwise TDNV under different noise ratios.
                        </figcaption>
                      </figure>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>

          <!-- Bias-variance Decomposition of Task Vectors Subsection -->
          <div style="margin-top: 3rem;">
            <h3 class="title is-4" style="color: #007bff; margin-bottom: 1.5rem;">Bias-variance Decomposition of Task Vectors</h3>
            
            <p style="font-size: 1.1em; color: #555; text-align: left; margin-bottom: 1.5rem;">
              Take Away: As the number of demonstrations K increases, we observe an intriguing phenomenon:
              <br>&nbsp;&nbsp;&nbsp;&nbsp;â€¢ Different tasks induce task vectors in distinct directions, yet each task follows a consistent direction.
              <br>&nbsp;&nbsp;&nbsp;&nbsp;â€¢ The variance within each task decreases.
              <br><br>
              Thus, we decompose the task vector into bias and variance components:
            </p>

            <div class="equation-box">
              $h_{t,i}(K) = \mu_t(\infty) + \underbrace{\mu_t(K) - \mu_t(\infty)}_{\text{bias}} + \underbrace{h_{t,i}(K) - \mu_t(K)}_{\text{variance}}$
            </div>

            <div class="columns is-centered">
              <div class="column is-fullwidth">
                <div class="content has-text-centered">
                  <div class="columns is-centered">
                    <div class="column is-6">
                      <figure class="image">
                        <img src="static/images/task_vector_pca_same_query_anno.png" alt="Task Vector PCA" style="width: 75%; border-radius: 8px; margin: 0 auto; display: block;">
                        <figcaption style="margin-top: 10px; font-size: 0.9em; color: #666;">
                          PCA visualization of task vectors from different tasks. As K increases, task vectors from different tasks become more separated while variance within each task decreases.
                        </figcaption>
                      </figure>
                    </div>
                    <div class="column is-4">
                      <div style="margin-bottom: 2rem;">
                        <figure class="image">
                          <img src="static/images/distance_vs_icl_length.png" alt="Distance vs ICL Length" style="width: 70%; border-radius: 8px; margin: 0 auto; display: block;">
                          <figcaption style="margin-top: 10px; font-size: 0.9em; color: #666;">
                            Decrease of bias as $\mathcal{O}(1/K)$.
                          </figcaption>
                        </figure>
                      </div>
                      <div>
                        <figure class="image">
                          <img src="static/images/variance_vs_icl_length.png" alt="Variance vs ICL Length" style="width: 70%; border-radius: 8px; margin: 0 auto; display: block;">
                          <figcaption style="margin-top: 10px; font-size: 0.9em; color: #666;">
                            Decrease of variance as $\mathcal{O}(1/K)$.
                          </figcaption>
                        </figure>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
            
            <p style="font-size: 1.1em; color: #555; text-align: left; margin-bottom: 1rem;">
              Decrease of bias:
            </p>
            <div class="equation-box">
              $\|\mu_t(K) - \mu_t(\infty)\|_2 \propto \mathcal{O}(1/K)$
            </div>
            
            <p style="font-size: 1.1em; color: #555; text-align: left; margin-bottom: 1rem;">
              Decrease of variance:
            </p>
            <div class="equation-box">
              $\|\mathbb{E}[(h_{t,i}(K) - \mu_t(K))^2]\|_2 \propto \mathcal{O}(1/K)$
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{jiang2025compression,
        title={From Compression to Expansion: A Layerwise Analysis of In-Context Learning},
        author={Jiang, Jiachen and Dong, Yuxin and Zhou, Jinxin and Zhu, Zhihui},
        journal={arXiv preprint arXiv:2505.17322},
        year={2025}
      }
  </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
             <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
